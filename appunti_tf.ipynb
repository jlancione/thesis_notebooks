{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["SHcRUxsrZdra","iXd2IkiTBF9R"],"authorship_tag":"ABX9TyM5MXbbWfQIC1lQUbEj3nff"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Da guardare ?\n","- tratto gli algoritmi di ottimizzaz come con ROOT ie a sentimento (learning_rate? arriva dla teoria!! sia lode)\n","- batch norm (qualitativam ok)\n","- pké si parli di API -> pké sto facendo parlare tensorflow e keras?\n","\n","https://keras.io/api/ c'è tutta la documentazione con esempi credo!!\n","\n","probab c'è anche 1 bella documentazione di tensorflow (con tnt di video?)\n","https://www.tensorflow.org/api_docs/python/tf e https://www.tensorflow.org/guide\n","\n","video del mio bro coreano: https://www.youtube.com/@Deeplearningai/playlists\n","\n","tutorial del mio altro bruh: https://www.youtube.com/playlist?list=PLhhyoLH6IjfxVOdVC1P1L5z5azs0XjMsb\n","\n","reference per python (?): https://realpython.com/\n","\n","MIT material (videos code and slides): http://introtodeeplearning.com/2023/index.html"],"metadata":{"id":"go38_m5l4jOX"}},{"cell_type":"markdown","source":["# Basics"],"metadata":{"id":"SHcRUxsrZdra"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6DhCvdOxL-N"},"outputs":[],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # per sbarazzarsi dei messaggi di info di tf\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.datasets import mnist\n","\n","# it may be necessary to import matplotlib.pyplot\n","\n","# to avoid GPU errors\n","#physical_devices = tf.config.list_physical_devices('GPU')\n","#tf.config.experimental.set_memory_growth(physical_devices[0], True)"]},{"cell_type":"code","source":["# Creiamo dei tensori\n","x = tf.constant([[2,3,4],[5,6,7]], shape=(2,3), dtype=tf.float32)\n","\n","y = tf.eye(4) # produce la matrice identità dla dimens desiderata\n","\n","# posso generarli da distribuzioni di probab e usando tf.range(start,limit,delta)\n","x = tf.cast(x,dtype = tf.float32) # per convertire tra i tipi\n","\n","x = tf.reshape(x, (6,))\n","indices = tf.constant([2,4])\n","x_ind = tf.gather(x,indices) # con x vettore e indices vettore di indici da estrarre\n","print(x_ind)\n","print(tf.shape(x))\n","x = tf.reshape(x,(2,3))\n","print(x)"],"metadata":{"id":"iolA0pKYxcBT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708431666791,"user_tz":-60,"elapsed":250,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"75ab403c-010a-4aa9-ecc5-5f9d879b6efb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([4. 6.], shape=(2,), dtype=float32)\n","tf.Tensor([6], shape=(1,), dtype=int32)\n","tf.Tensor(\n","[[2. 3. 4.]\n"," [5. 6. 7.]], shape=(2, 3), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["le operaz le fa già da sé element-wise\n","`z = tf.tensordot(x,y,axes=1)` fa il prodotto scalare (ma è in grado di fare molto di +), in sost credo che axis mi dica su qle dimensione sto agendo\n","`tf.matmul(a,b) = a @ b` per le moltipl tra matrici\n","\n","l'indexing funziona come il solito python, `x[2:3]` con 2 incluso e 3 escluso, `:` da solo significa tutti e posso ancora aggiungere un altro colon `:step`\n","\n","e qsti giochini sugli indici si estendono chiaramente a ogg di dimensione maggiore"],"metadata":{"id":"w3-Ci4sHTEYQ"}},{"cell_type":"markdown","source":["# Object Oriented Python\n","classes are data structures (ie blueprints) and the objects are their instances"],"metadata":{"id":"ondukOTNtvVN"}},{"cell_type":"code","source":["class ClExample(): # this is the convention for class names\n","# between the () goes the parent classes in case of inheritance\n","# qua (ie fuori da __init__) dichiaro e assegno le class variables (qle =i per tutte le instances)\n","\n","  def __init__(self, attribute1, attribute2): # stands for initialize (è il costruttore direi), self as first attribute is mandatory\n","  # qua sto contemporaneam dichiarando gli attributi e utilizzando le corrisp funzioni set\n","  # metodi dla classe (ie le funzioni precipue dla classe)\n","  # anche .__init__ è 1 method (qndo crei 1 instance stai in sostanza chiamando il metodo costruttore ie ClExample.__init__(param))\n","\n","  # super(nel caso di 1 gerarchia + complicato devo metterci i riferimenti per chiamare il costruttore giusto).__init__(qua ci vanno gli argom richiesti per inizializz l`ogg parente (senza self neh))\n","    self.attr1 = attribute1\n","    self.attr2 = attribute2\n","\n","  def __str__(self):\n","    return 'qlo che vuoi compaia qnd printi 1 instance'\n","# tutti i methods che intendi def devono avere come 1o arg self\n"],"metadata":{"id":"io_jVVfbt5Bw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["subclasses per blocchi di layers:\n","- eredito da layers.Layer (come costruttore `super(nome_del_blocco, self).__init__()` pké tnt il costrutt di layers nn richiede argom)\n","- come attributi metto i layer elementari che userò\n","- 1 metodo `call` (è sicuram 1 metodo dla classe parente che sto sovrascrivendo… come puoi ben capire ce ne sono altri… es build) dove costruisco il blocco nel modo functional ma senza gli `(x)` al fondo, e qsto metodo returna l'ultimo strato/il suo output?\n","(dentro il `call` posso chiamare print e lui mi stampa qlo che mi interessa mentre allena, ie ogni volta che viene chiamato qlo strato)\n","- se voglio definire 1 classe di modelli (al posto che definirlo in 1 funz posso farlo in 1 classe) la parent class dev'essere `keras.Model`, per il resto è tutto = (ma d'altronde i modelli nn sono forse blocchi di layer con qlche metodo in +…)"],"metadata":{"id":"_cbkudgCRAQo"}},{"cell_type":"markdown","source":["# Data and basic model\n","overgap between training e test accuracy is a symptom of **overfitting** -> solution: regularization\n","\n","if the distribution of the inputs changes it maybe needed to retrain the model (magari la legge e la stsa ma i dati sono finiti fuori dal campo di misura precedente e dnq nn siamo in grado di dare previsioni accurate presumibilmente) this is called *covariance shift* (->batch normalization)\n","\n","altra idea è: gioca con i param per capire trovare compromessi (abbast epoch ma nn troppe, l'optimaizer che lavora meglio…)\n","\n","NN tutti i param sono trainable -> e certe cose (ad es la BatchNorm) si comportano in modo diverso a 2a se sono in training o testing -> è il caso di settare `training=True/False` nei param del model"],"metadata":{"id":"0tduPGXBZVsE"}},{"cell_type":"code","source":["# qlo che esce da qua (e con cui lavoriamo dopo) sono array numpy, ma ci va bene = (converte da solo)\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","# dobbiamo appiattire il tutto\n","x_train = x_train.reshape(-1, 28*28).astype(\"float32\")/255.0 # 28*28 sono i pixel dl'immagine, -1 serve a nn toccare la 1a dimensione\n","# / 255.0 è normalizzaz a 1, la macchina ci mette - (?)\n","x_test = x_test.reshape(-1, 28*28).astype(\"float32\")/255.0\n","print(tf.shape(x_train))"],"metadata":{"id":"KEiGEPTbUB1s","executionInfo":{"status":"ok","timestamp":1708463537817,"user_tz":-60,"elapsed":1432,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"faed6e3f-a421-4daf-e339-93a6706074b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n","tf.Tensor([60000   784], shape=(2,), dtype=int32)\n"]}]},{"cell_type":"code","source":["# SEQUENTIAL API (1 input maps to 1 output)\n","model = keras.Sequential(\n","    [\n","        keras.Input(shape=(28*28)), # qsto è in realtà il primo strato (che ha il compito di accogliere gli input), qli dopo sono gli hidden layers\n","        layers.Dense(512, activation = 'relu'), # Dense significa che tutti in nodi sono collegati con tutti\n","        layers.Dense(256, activation = 'relu'),\n","        layers.Dense(10) # qsto è l'output con le 10 possibili cifre\n","    ]\n",")\n","print(model.summary())\n","# è 1 debugging tool (come sempre d'altronde i print), se uso model.add() per aggiungere i layers 1 alla volta posso printare il summary dopo ogni strato"],"metadata":{"id":"FRlVG9b8lEBM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708428652364,"user_tz":-60,"elapsed":264,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"793520bf-5dd3-4a44-80c0-b1a20e5afe5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_12 (Dense)            (None, 512)               401920    \n","                                                                 \n"," dense_13 (Dense)            (None, 256)               131328    \n","                                                                 \n"," dense_14 (Dense)            (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 535818 (2.04 MB)\n","Trainable params: 535818 (2.04 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# tell keras HOW to train the model\n","model.compile(\n","    # set softmax activation for the output layer through the loss function (with the from_logits argument), otherwise just set it in the model\n","    # Sparse is needed because the labels are just numbers (otherwise avremmo bisogno di 1 hot encodings)\n","    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","    metrics=[\"accuracy\"] # what we keep track of during the training\n",")\n","\n","# h specifichiamo gli aspetti pratici del training\n","model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2) # con verbose=2 printa dopo ogni epoch\n","\n","# HYPERPARAMETERS (?): learning rate, lambd\n","# 1 epoch = 1 passaggio dl'intero dataset per il training algorithm\n","# batch size = dopo qnti sample aggiornare i param del modello/verificare i risultati(?)\n","model.evaluate(x_test, y_test, batch_size=32, verbose=2) # qua epoch = 1 (mica ci interessa controllare + volte…)"],"metadata":{"id":"Z7aTXuuTnOt3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708429459159,"user_tz":-60,"elapsed":83684,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"a2f89117-0dab-4598-a23a-2f23c13d5614"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 - 16s - loss: 0.1307 - accuracy: 0.9632 - 16s/epoch - 8ms/step\n","Epoch 2/5\n","1875/1875 - 17s - loss: 0.0544 - accuracy: 0.9829 - 17s/epoch - 9ms/step\n","Epoch 3/5\n","1875/1875 - 15s - loss: 0.0382 - accuracy: 0.9877 - 15s/epoch - 8ms/step\n","Epoch 4/5\n","1875/1875 - 16s - loss: 0.0301 - accuracy: 0.9902 - 16s/epoch - 9ms/step\n","Epoch 5/5\n","1875/1875 - 17s - loss: 0.0252 - accuracy: 0.9921 - 17s/epoch - 9ms/step\n","313/313 - 1s - loss: 0.0808 - accuracy: 0.9787 - 851ms/epoch - 3ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.08082181960344315, 0.9786999821662903]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["#import sys\n","#sys.exit()\n","# serve per terminare il programma/script in a graceful way :)\n","# it raises a SystemExit exception, signaling an intention to exit the interpreter\n","# ma sì serve per nn fargli tirare dritto ad eseguire qlo che viene dopo (è 1 va a caghè)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73LK7ExKhX23","executionInfo":{"status":"ok","timestamp":1708415889986,"user_tz":-60,"elapsed":320,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"b10ea93e-15aa-4f58-a137-d1452bca99ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 512)               401920    \n","                                                                 \n"," dense_4 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_5 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 535818 (2.04 MB)\n","Trainable params: 535818 (2.04 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# FUNCTIONAL API\n","inputs = keras.Input(shape=(28*28))\n","x = layers.Dense(512, activation='relu', name = 'first_layer')(inputs)\n","x = layers.Dense(256, activation='relu', name = 'second_layer')(x) # nn so bene come interpretare qsta roba pké h stiamo definendo (e sovrascrivendo x) ma dopo dovremo usarlo…\n","outputs = layers.Dense(10, activation='softmax')(x)\n","model = keras.Model(inputs=inputs, outputs=outputs) # qua stiamo raccattando tutto {}\n","\n","# il vantaggio dla functional è che posso biforcare alla fine: outputs1 = …(x) e outputs2 = …(x), in keras.Model(…, outputs=[outputs1,outputs2])\n","# qua sotto in compile posso avere loss function e metriche diverse (uso 1 lista come 1a)\n","\n","model.compile(\n","    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), # pké abbiamo già specificato l'attivazione degli outputs, clipnorm = 1 evita exploding gradient problems\n","        optimizer=keras.optimizers.Adam(learning_rate=0.001), # learning rate\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n","model.evaluate(x_test, y_test, batch_size=32, verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Av_1H_Jslo9n","executionInfo":{"status":"ok","timestamp":1708420282316,"user_tz":-60,"elapsed":84011,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"35636f05-6921-4e51-a298-81fb11fe4fcc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 - 12s - loss: 1.0014 - accuracy: 0.7725 - 12s/epoch - 7ms/step\n","Epoch 2/5\n","1875/1875 - 12s - loss: 0.4465 - accuracy: 0.8857 - 12s/epoch - 6ms/step\n","Epoch 3/5\n","1875/1875 - 12s - loss: 0.3659 - accuracy: 0.9007 - 12s/epoch - 6ms/step\n","Epoch 4/5\n","1875/1875 - 11s - loss: 0.3279 - accuracy: 0.9096 - 11s/epoch - 6ms/step\n","Epoch 5/5\n","1875/1875 - 14s - loss: 0.3037 - accuracy: 0.9155 - 14s/epoch - 7ms/step\n","313/313 - 1s - loss: 0.2826 - accuracy: 0.9212 - 819ms/epoch - 3ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.28261861205101013, 0.9211999773979187]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["1 volta che ho 1 model posso lavorarci sopra…\n","- accedere ai layer: `model.layers[index]` con le solite convenzioni, `model.get_layer(name='layer_name')` (dove il nome lo setto qnd definisco il layer)\n","- estrarre input e output: `layer.inputs` e `layer.output` (dove chn layer intendo che ne hai richiamato 1 in qlche modo)\n","… e ad esempio usare il suo output per allenare qlche layer a valle (per farlo devo `model.trainable = False` sul modello che voglio che tenere fisso, altrimenti posso agire sui singoli layers con `layer.trainable = False`, dove con layer intendo che ne hai richiamato uno)"],"metadata":{"id":"0JRsO1DBnnO0"}},{"cell_type":"code","source":["model = keras.Model(inputs=model.inputs,\n","                    outputs=[model.layers[-2].output]) # con il -2 stiamo andando a vedere l'ultimo hidden layer\n","# che cosa sta succedendo qua? (per debuggare tipicam) sto definendo 1 nuovo modello = al mio (e con gli stsi valori dei param direi) ma a cui tolgo strati dal fondo\n","# poi per andare a vedere nn devo far altro che far girare qsto nuovo modello su dle x a piacere\n","\n","# accedere ai layers: model.layers[index] (dove il sistema di indici funziona come per i vettori) il .output mi sfugge\n","#                     model.get_layer(name=\"layer_name\")\n","\n","feature = model.predict(x_train) # puoi far girare il modello su qlo che ti pare\n","print(tf.shape(feature))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6PMtrp1qz7_","executionInfo":{"status":"ok","timestamp":1708430873690,"user_tz":-60,"elapsed":6008,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"f2519561-eded-4e46-86c2-17baab2fd396"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1875/1875 [==============================] - 4s 2ms/step\n","tf.Tensor([60000   512], shape=(2,), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["Posso lavorare indistintam con array di ogg ie nel mio definire `model` posso tranquillam mettere `output=[layer.output for layer in model.layers]` e ottenere 1 array/tensor di modelli!"],"metadata":{"id":"LxY99J3cuf5M"}},{"cell_type":"markdown","source":["# Convolutional Neural Networks"],"metadata":{"id":"iXd2IkiTBF9R"}},{"cell_type":"markdown","source":["al solito con la convoluzione si passa una maschera/(filtro/kernel) (dove la maschera è deputata a riconoscere qlche tipo di pattern)/sistema di pesi (che poi sono i param da tweakare con l'allenamento, ie è il training che insegna al modello i pattern da riconoscere!) e lo si fa scorrere (puoi chiaram farla scorrere in modi fantasiosi es a passi di 2 o che, in qsto modo cambia la dimensione dl'output) prendendo i prodotti element-wise e poi sommando tutto, l'output è dimensionalm + piccolo dl'input\n","\n","- **padding**: aggiungere una cornice per nn perdere troppo info dai bordi (la maschera ci passa sopra poco rispetto a celle/.i + centrali) -> permette di avere output e input con stsa dim (ie padding='same' otherwise padding='valid' se NN voglio padding)\n","\n","1 dle dimensioni dl'input può essere associata ai **channel** (tipicam RGB), per convenz si chiede che il filtro abbia lo stso numero di channel (ie che il filtro nn si muova tra i channel, che ha molto senso se intendiamo che i channel portino info distinte) -> dnq l'output è piatto (ie perdiamo totalm/si annulla la dimensione a cui corrisp i channel)\n","\n","in 1 passaggio possiamo applicare 2 maschere e ottenere 1 output nn piatto (ie stiamo cercando di riconoscere 2 pattern/features in contemporanea -> qsti saranno i channel dl'output! ie 1 pallozzi che rappresentano i neuroni! ie 1 pallozzo <-> 1 channel)\n","\n","- **max pooling** è 1 step di pura computazione (ie nn ci sono parametri) e semplicem funziona bene. Faccio correre 1 maschera (pool) che ha il compito di estrarre il numero maggiore, nulla + (l'idea è che se in qla regione ho trovato il pattern allora c'è almeno 1 numero grande, dnq se trovare il pattern è il mio obiettivo qsto chiaramente elimina le info nn interessanti riducendo le dimensioni dl'output dl layer)\n","\n","`layers.Conv2D(num di channel dl'output ie num di kernels, dim del kernel, padding='', activation)`\n","\n","`layers.MaxPooling2D(pool_size= dim dla maschera)`\n","\n","ecco che h è utile il `print(model.summary())` per tenere traccia dle varie dimensioni e parametri\n","\n","qsto genere di layer qua agisce su dati organizzati sotto forma di tensori, se voglio poi passare a 1 strato Dense (ad es) devo appiattire tutto con `layers.Flatten()`\n"],"metadata":{"id":"vzraTiJe44eO"}},{"cell_type":"code","source":["x_train = tf.reshape(x_train,(-1,28,28))\n","x_test = x_test.reshape(-1,28,28) # ok quick refresh: applicare 1 metodo (ie le cose con il .) nn agisce modificando l'ogg è indisp riassegnare\n","print(tf.shape(x_train))\n","print(tf.shape(x_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkFuMUkmckm5","executionInfo":{"status":"ok","timestamp":1708463554997,"user_tz":-60,"elapsed":254,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"b27e7d3c-711a-42b2-fd70-b803e4beb4dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([60000    28    28], shape=(3,), dtype=int32)\n","tf.Tensor([10000    28    28], shape=(3,), dtype=int32)\n"]}]},{"cell_type":"code","source":["def convnet_model():\n","  inputs = keras.Input(shape=(28,28,1)) # qsto 1 finale è indisp *******>>>> riguardami ;)\n","  x = layers.Conv2D(32,(3,3),padding='valid',activation='relu')(inputs)\n","  x = layers.MaxPooling2D(pool_size=(2,2))(x)\n","  x = layers.Conv2D(16,3)(x)\n","  x = layers.BatchNormalization()(x)\n","#  x = keras.activations.relu(x) # nn è 1 layer, stiamo settando 1 opzione del layer e va fatto dopo BatchNorm\n","  x = tf.nn.relu(x) # credo sia analogo al metodo sopra\n","  x = layers.Flatten()(x) # qsto invece sì che è 1 layer\n","  x = layers.Dense(16,activation='relu')(x)\n","  outputs = layers.Dense(10)(x) # l'activation di qsti sarà settata dopo\n","\n","  model = keras.Model(inputs=inputs, outputs=outputs)\n","  return model\n","\n","model = convnet_model()\n","\n","model.compile(\n","    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), # pké abbiamo già specificato l'attivazione degli outputs\n","        optimizer=keras.optimizers.Adam(learning_rate=0.001), # learning rate\n","    metrics=[\"accuracy\"]\n",")\n","\n","model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n","model.evaluate(x_test, y_test, batch_size=32, verbose=2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PErRDSGVzxV","executionInfo":{"status":"ok","timestamp":1708463701305,"user_tz":-60,"elapsed":144275,"user":{"displayName":"Jacopo Lancione","userId":"18417370859735724508"}},"outputId":"e879292d-723f-4929-a9a4-e0a99b71f891"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1875/1875 - 19s - loss: 0.1729 - accuracy: 0.9466 - 19s/epoch - 10ms/step\n","Epoch 2/5\n","1875/1875 - 18s - loss: 0.0508 - accuracy: 0.9842 - 18s/epoch - 10ms/step\n","Epoch 3/5\n","1875/1875 - 18s - loss: 0.0385 - accuracy: 0.9877 - 18s/epoch - 10ms/step\n","Epoch 4/5\n","1875/1875 - 18s - loss: 0.0300 - accuracy: 0.9902 - 18s/epoch - 9ms/step\n","Epoch 5/5\n","1875/1875 - 18s - loss: 0.0243 - accuracy: 0.9920 - 18s/epoch - 10ms/step\n","313/313 - 1s - loss: 0.0331 - accuracy: 0.9885 - 1s/epoch - 4ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.03313692659139633, 0.9884999990463257]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Overfitting & Regularization\n","obb: shrinkare i weights (ie diminuire i gdl o perlomeno nn lasciarli liberi del tutto)\n","anche se nn hai overfitting ha cmq senso provare a condire il modello con 1 po' di regularization"],"metadata":{"id":"jUwp-j_b5pUq"}},{"cell_type":"markdown","source":[" high variance cases leadsto very nn linear decision boundaries (pké arrivo a dei boundaries sartoriali sul training sample ma se la popolazione è sul serio con 1 high variance sicuramente nn sono riuscito ad identificare gli effettivi boundaries pké facilmente ci sono molte variazioni nn previste dai dati di allenam… la vera soluz per casi di high variance è radunare 1 training set + grande e nn ridurre i gdl ma si fa ql che si può) -> pochi dati? introduci del dropout se no overfitti\n","\n","(anche la **BatchNorm** agisce 1 po' da regularization a qnt pare, e si comporta in modo diverso se sto allenando la rete o la sto testando?)\n","- **L2** (regularization si riferisce alla norma euclidea/di Frobenius dla matrice dei pesi) e l'idea è qla di aggiungere 1 termine (con qsta norma e pesato da 1 param) alla loss function che diminuisca gli incrementi nei parametri (dandogli - modo di aggiustarsi) -> also referred as weight decay\n","`kernel_regularizer=regularizers.l2(param)` va aggiunto in tutti i layer dove vuoi che avvenga (immagino la sintassi sia diversa se nn si tratta di 1 convolutional layer ma ci siamo intesi)\n","\n","- **Dropout** ad ogni iteraz ti faccio sparire 1 fraz fissata (hyperparam) di unità da 1 livello. Le unità del liv successivo allenano i pesi con meno efficacia pké nn possono fare affidam su ness1 di loro (dandogli troppo importanza) -> nn si riesce a sfruttare bene qsti pesi (e noi vogliamo diminuire i gdl qndi ottimo!)\n","`x = layers.Dropout(perc di link da buttare)(x)` nla  functional API -> ci vogliono + epochs per allenam decentem (fa + fatica poverina…)"],"metadata":{"id":"F3hEIwO9tnGE"}},{"cell_type":"markdown","source":["Data augmentation is great to avoid overfitting!! the model isn't able to memorize the training set because it's enormous and it changes! so the idea is not to make this augmentation static but to randomize it\n","(this augmentation proceeds in parallel during the training, and you may want to make in part of your model (does it have any sense if you then want to serialize it?))"],"metadata":{"id":"lxp2xkoutqND"}},{"cell_type":"markdown","source":["# Save the model & Datasets"],"metadata":{"id":"i-5LbkCdbOR_"}},{"cell_type":"markdown","source":["qsta roba è molto comoda per spezzare il training e riprenderlo più tardi\n","(devo dare i nomi ai pesi se mi sono costruito i layer custom, fai occhio)\n","- save & load model weights:\n","`model.save_weights('path')` and `model.load_weights('path')` (e nn devi assegnare qsta roba ie nn devi fare `model = model.load…`) tra qnd salvi e carichi devi utilizzare la stsa implementaz (sequential/functional) altrim ti fa 1 pernacchia\n","\n","- save & load entire model (serialization): saves weights, model architecture (dnq qlo che esporto può avere vita indip dal codice in cui nasce il modello), training config (the parameters of `model.compile()` and what they have learned, es l'optimizer ha dei parametri interni che si aggiornano?)\n","`model.save('path')` (potrebbe comparirti 1 errore ma nn dovrebbe essere 1 pb) and `model = keras.model.load_model('path')` hai tutto pronto ie devi solo fittare\n","\n","Per utilizzare 1 modello già allenato di keras `keras.applicantions.nome_del_pretrained_model()`"],"metadata":{"id":"BxT7RIhQOfMB"}},{"cell_type":"markdown","source":["# Callbacks\n","`keras.callbacks.nome_del_callback_che_ti_interessa` -> guarda la documentazione!\n","\n","posso agire sul comportam del modello nle fasi di training o evaluation (ad es posso scegliere di stoppare e salvare tutto qndo raggiungo 1 certa accuracy)\n","\n","devo scegliere la callback che mi interessa settarne i param e poi darla in argom al `model.compile` o `model.fit` -> si possono avere + callbacks qndi mi aspetto che il parametro callback in qsti 2 metodi accetti 1 lista\n","\n","se voglio scrivermi io 1 callback definisco 1 classe che eredita da `keras.callbacks.Callback` (e ho già dei bei methods predefiniti che permettono di scegliere il momento in cui voglio intervenire)"],"metadata":{"id":"2nO5MOXBeBfo"}},{"cell_type":"markdown","source":["# Custom things\n","to act on fit and compile methods you have to keep track of (ie record) the operations in order to compute the gradients for the backprop\n","\n","se voglio allenare il model in qlche modo particolare posso agire su `model.fit` creando 1 subclass di `keras.Model` e cambiando i methods relativi all'allenamento (ie ridef il method `train_step` che è sicuram usato under the hood da `model.fit`), altrim mi scrivo a mano il training loop e nn uso il metodo `.fit`"],"metadata":{"id":"9PFa9urenDQw"}},{"cell_type":"code","source":["x = tf.constant(3) # sarà poi la valutazione\n","with tf.GradientTape() as tape: # qsto statement serve qndo lavoro con unmanaged resources\n","  # (qlo che sto scrivendo è praticam 1 mini file! che dopo eseguirò qnd chiamerò gradient!)\n","  # operations you want to record (write them in symbols, as in Scipy I think)\n","  # training variables are automatically recorded\n","  # you can manually record a tensor object if you \"watch\" it\n","  # pké lui si ricordi/registri è suffic che 1 sola variab sia watchata\n","  tape.watch(x)\n","  y = x * x\n","tape.gradient(y,x) # derivo risp a x e valuto (nota l'indentazione… siamo fuori dal filmino/tape e lo stiamo usando)\n","# appena eseguo .gradient() si dimentica di tutto e butta il tape (se ne ho ancora bisogno: .GradientTape(persistent=True))"],"metadata":{"id":"Y7hOLcAhGVhD"},"execution_count":null,"outputs":[]}]}